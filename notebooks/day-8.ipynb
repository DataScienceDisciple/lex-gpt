{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def truncate_text_to_max_tokens(text: str, max_tokens: int = 4000, encoding_name: str = \"gpt-3.5-turbo-16k\") -> str:\n",
    "    \"\"\"Truncate text from the file to a maximum number of tokens.\"\"\"\n",
    "    \n",
    "    \n",
    "    current_num_tokens = num_tokens_from_string(text, encoding_name)\n",
    "\n",
    "    if current_num_tokens > max_tokens:\n",
    "        print(f'Text truncated, num tokens: {current_num_tokens}')\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "        token_list = encoding.encode(text)\n",
    "        truncated_token_list = token_list[:max_tokens]\n",
    "        truncated_text = encoding.decode(truncated_token_list)\n",
    "    else:\n",
    "        print(f'Text not truncated, num tokens: {current_num_tokens}')\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode - Balaji Srinivasan_ How to Fix Government, Twitter, Science, and the FDA _ Lex Fridman Podcast #331 Segment - Fixing the FDA (3_25_06-4_56_14).txt\n"
     ]
    }
   ],
   "source": [
    "input_directory = 'transcripts'\n",
    "exceeding_limit = {}\n",
    "for filename in os.listdir(input_directory):\n",
    "    full_path = os.path.join(input_directory, filename)\n",
    "    with open(full_path) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    current_tokens = num_tokens_from_string(text, \"gpt-3.5-turbo-16k\")\n",
    "\n",
    "    if current_tokens > 16000:\n",
    "        exceeding_limit[filename] = current_tokens\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Episode - Balaji Srinivasan_ How to Fix Government, Twitter, Science, and the FDA _ Lex Fridman Podcast #331 Segment - Fixing the FDA (3_25_06-4_56_14).txt': 19260}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceeding_limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a podcast summarization expert. Your outputs are easy to read-through, concise and actionable.\n",
    "\n",
    "As an input you will receive a transcript of the podcast section.\n",
    "\n",
    "Your task is to create concise and in-depth summary based on the transcript that you will receive. \n",
    "Compress as little information as possible.\n",
    "If there is any information that should be added, please do so. \n",
    "\n",
    "Your answer will be presented to an audience interested in science, technology, artificial intelligence and self-improvement.\n",
    "\n",
    "Here is the transcript:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "SUMMARY IN BULLET POINTS (UP TO 15 POINTS):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_summarize_file(chain, full_path, save_path):\n",
    "    with open(full_path) as f:\n",
    "        text = f.read()\n",
    "    text = truncate_text_to_max_tokens(text)\n",
    "    doc = [Document(page_content=text)]\n",
    "    output_summary = await chain.arun(doc)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(output_summary)\n",
    "\n",
    "async def summarize_files_from_directory(input_directory, output_directory, prompt_template, model_name=\"gpt-3.5-turbo-16k\", batch_size=30):\n",
    "    llm = ChatOpenAI(model_name=model_name)\n",
    "    BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
    "                                         input_variables=[\"text\"])\n",
    "    chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "    # Get a list of valid files \n",
    "    # - Those not starting with a dot\n",
    "    # - Those not already summarized\n",
    "    valid_files = [\n",
    "        filename for filename in os.listdir(input_directory) \n",
    "        if not filename.startswith('.') \n",
    "        and not os.path.exists(os.path.join(output_directory, f'(Summary) {filename}'))\n",
    "    ]\n",
    "\n",
    "    print(f\"Files to summarize: {len(valid_files)}\")\n",
    "    \n",
    "    # Process files in batches\n",
    "    for i in range(0, len(valid_files), batch_size):\n",
    "        batch = valid_files[i: i + batch_size]\n",
    "        tasks = []\n",
    "\n",
    "        for filename in batch:\n",
    "            full_path = os.path.join(input_directory, filename)\n",
    "            save_path = os.path.join(output_directory, f'(Summary) {filename}')\n",
    "            print(f'Summarizing: {filename}')\n",
    "            task = asyncio.create_task(async_summarize_file(chain, full_path, save_path))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # Wait for current batch to complete before proceeding to the next\n",
    "        await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'transcripts'\n",
    "output_dir = 'summaries'\n",
    "\n",
    "await summarize_files_from_directory(input_dir, output_dir, prompt_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
